# crane

So, this is basically just this idea I've had kicking around evolving in my head for a while ever since I first started learning C, about what I'd want a better compiled language to look like. Some of it's *purely* theoretical, and may have broad unrecognized (or just impractical) gaps in its epistemology; ultimately, I suspect the final product would end up looking something like [Nim](https://nim-lang.org/). (Many of the ideas in it took shape after looking at Lua, Haskell, Tcl, Bash, and Lisp.)

## Parsing

The parse structure would be *very* simple, outputting a tree where all the leaf nodes are simply byte-sequence string tokens (which then essentially represent S-expressions).

- Each new line, unless the line begins with a colon (`:`) character, starts a new branch (ie. implicit parentheses): this branch continues until the next line at the same, or lesser, level of indentation. (This would eliminate the need for a *lot* of the awkward redundant parens you see in most Lisp code.) Lines that begin with a colon (followed by whitespace) are treated as continuations.
- Any non-alphanumeric character outside of quotes, including non-flow-controlling-whitespace or a normally-starting quote, can be backslash-escaped as a single-character literal, Bash-style.
- Standard C backslash-escape characters (like `\n`) can be used outside double-quoting. Octal escape sequences (if they're permitted at all, which I'm not committed to) would be prefixed `\o` (lowercase "o"); the sequence `\0` represents no text (so `null\0sleep` is equivalent to the token `nullsleep`, and `\0` by itself represents the empty string as a bare token).
  - A syntax for null strings as a bare token is important, since `""` would be equivalent to `(\"\" \0)`, and you'd need a notation to represent that. I'd thought about making the empty string equivalent to nil (which may be equivalent to an empty list), but I decided that that would be too magic-y (suddenly recursive constructs that process tokens would need to check for lists or something and, just, yeesh).
- String syntax like `"Hello World"` would produce a construct equivalent to `(\"\" Hello\ World)`, ie. the token `""` followed by the token `Hello World`. Double-quotes accept backslash escape sequences like `\n` for a newline (and borrowing Lua's `\z` to eat all following whitespace): single-quotes interpret everything until the next single quote (including backslashes) literally (so backslash, as a token, can be written as either `\\` or `'\'`).
  - Single-quoting *may* also permit a construct where `'this'example` is equivalent to `thisexample`, to allow returning to a backslash-escape-permitting context (which would allow a single-quoted string to contain a single quote): I'm not sure if that's called for, though. (This would allow for a single-quoted string, ie. `(\'\' \')`, to be spelled `''\'`.)
- Tokens starting with a non-letter character (if not backslash-escaped or the first token of a branch) would be treated as infix, ie `1 + 1` produces `(\+ 1 1)`
  - This may be too loose: what might make more sense would be to have this for only tokens consisting *exclusively* of punctuation characters and/or or Haskell-style backticked tokens, with the first character of punctuation determining whether the infix is left- or right-associative.)
- Most other non-letter characters *within* a token are considered just part of the token (eg. `M*A*S*H` would be interpreted as one token)
- Consecutive dots and colons count as their own tokens without having to be separated by whitespace, unless the token starts with a digit (so `foo.bar` turns into `(\. foo bar)` but `3.14159` and `4:20` stay as they are).
- Square braces work like infix operators (and don't have to be separated by whitespace), but with the second argument being the one *within* the braces: `foo[bar]` becomes `(\[] foo bar)`
- Curly braces work similarly: `{foo bar baz}` becomes `(\{} foo bar baz)`
- The long-string syntax would probably be more like Bash heredocs than Python's triple-quotes or Lua's any-number-of-equal-signs-between-two-square-braces, with the delimiter working as a second parameter (which would generally be ignored): I'm thinking `[[here stringhere` evaluating to `(\[[ string here)`. (I've got some ideas around including end braces after the start / end token, which could be used to distinguish whether indentation / first newlines should be discarded, but it'd require some code blocks to explore and I don't have any strong feelings about the specifics)
- Actually, I'll just let heredocs be raw: for a long-string syntax that eats leading-space, I'm thinking that there can be a rule where a colon (`:`) at the end of a line starts a YAML-style text block that ends after the first line not indented (with characters after the colon controlling trimming/folding/chomping). 
- The original idea I had for comment characters - keeping in mind that I was just learning C - was that it'd have both `//` and `#`, where `#` would be for comments oriented toward *tooling*. These days, I think that's a dumb idea, but I'm not sure what character I'd want to reserve / cut out of the character set for tokenization: honestly, I'm leaning toward Lisp's `;`, as there aren't many other uses for semicolons in this model (and it'd go nicely with having colons as a reserved character for flow alongside parentheses).
  - This also introduces the possibility for a nice practice where every line of code should end in a semicolon, and everything to the right of the code should be a literate-programming-style running narrative for the code's intent.

## Compiling

Compiling would evaluate the parsed tree in an environment halfway between Lua and Haskell: the only types are boolean, number, table, string, function, nil, and maybe some kind of userdata (or coroutine), but all functions are curried, and the environment is built as a Prelude.

The environment would be a table with all global names in it, and would start with the primitive functions that can't be defined (ie. the kind of things that Lua provides in its Standard Library) preloaded (the Skyhook). This includes constructors like `function` and basic operations like addition (`+`) and indexing (`[]` and/or `.`).

At the head of every branch, the interpreter goes into the global environment table and calls the corresponding function with the following token (or result of a branch) in the branch (if there is any), and so on until the branch is consumed. (By default, tokens return themselves.)

The environment would have a stack where further lexical contexts inherit non-conflicting names and that whole thing - basically, there'd be the *private* environment, which is generally encapsulated with scope in other languages, and the *public* environment, which I've never really seen in a stack-based context (ie. not just global clobbering) outside of shell programming, to be honest. (It feels weird to say that a concept useful for *functional* programming could come from experience with *Bash*, but there it is.)

(This "public environment" bit where we override the definitions of core functions for calls in lower contexts so they produce new symbols appropriate for their calling contexts is, like, a whole thing that I don't really know that I've ever seen, and feels like it might be super wonky in practice, but is also a core aspect of this design - it's necessary so "=" can both mean "assign this macro function to this name" and "allocate a block of RAM for this integer" for the same function being evaluated / transformed in radically different contexts, one producing the actual value and one producing the corresponding machine instructions. I guess the object-orienting world considers this "inheritance", but, like... I think this is backwards from that)

Functions would handle interpreting tokens as variable names or strings or numbers or whatever as appropriate to their context. How numbers get parsed into numbers while strings that contain numbers don't is something I imagine being handled by the context stack I just described: in contexts where strings would be differentiated from names or numbers, the stringification functions (which I'm thinking would have the different `"` and `'` character aliases but all fall under the name `string` or `__string`) that, at a base level, just return the token as a string, would be overridden with one that decorates into something like a table where `{type='string', value='foo'}`, whereas numbers would be parsed by whatever number parsing rule makes sense, deeper expressions would return some kind of object that represents the return value of a function call or whatever (the output of the function those tokens would be defined as), and bare tokens (which would be the only ones coming back as raw strings) would, I guess, be interpreted however maes sense (maybe the value by that name will be inlined as an object representing a const).

At this point, you'd have preludes that would define the macros for converting whatever expressions into whatever machine code (or JavaScript or whatever) following whatever rules. The compiler would run through these to fill out the environment that could be said to define the actual "language" (which would be, like I said, something like Nim, with lots of nice advanced language features that are all optional if you just want to write something super hard-coded and stuff), then it'd go through the whole "compiling" stuff that I fully realize would make up 99.9% of the actual work of developing this (which is why this has all stayed a *thought experiment* for me instead of a project I've actually *undertaken*).
